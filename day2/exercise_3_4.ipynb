{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding a Sequence\n",
    "\n",
    "**So far we have seen how to train a HMM.**\n",
    "\n",
    "**Now we will focus on, once trained, how to make predictions efficiently with a HMM**\n",
    "\n",
    "\n",
    "Given the learned parameters and a new\n",
    "observation sequence $x = x_1\\ldots x_N$, we want to find the sequence of hidden states $y^* = y_1^* \\ldots y_N^*$ that \"best\" explains it.\n",
    " This is called the **decoding problem**. \n",
    " \n",
    " There are several ways to define what we mean by the \"best\" $y^*$, depending on our goal: for instance, we may want to minimize the probability of error on each hidden\n",
    "variable $Y_i$ (posterior decoding), or we may want to find the best assignment to the sequence $Y_1\\ldots Y_N$ as a whole (viterbi decoding). \n",
    "Therefore, finding the best sequence\n",
    "can be accomplished through different approaches:\n",
    "\n",
    "- ** posterior decoding** or **minimum risk decoding**\n",
    "\n",
    "    This approach selects, at each step $i$, the state that maximizes the conditional probability of $Y_i$ given all the visible sequence. Notice that the state sequence that this approach reaches is not necesary the one that maximizes the probability of the whole sequence and state sequence.\n",
    "    \n",
    "\\begin{equation}\n",
    "y_i^* = \\arg \\max_{y_i \\in \\Lambda} P(Y_i=y_i | X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{equation}\n",
    "\n",
    "- ** Viterbi decoding**\n",
    "\n",
    "    This approach choose the sequence of states that, overall, has the highest probability.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y^* &=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N | X_1=x_1,\\ldots,X_N =x_N)\\nonumber\\\\\n",
    "&=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N, X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{eqnarray}\n",
    "\n",
    "### Unfolding all state sequences: trellis representation\n",
    "Both previous approaches, viterbi decoding and posterior decoding, rely on dynamic programming and make use of the\n",
    "independence assumptions of the HMM model. Moreover, they use an alternative representation of the HMM called a trellis. \n",
    "\n",
    "A trellis unfolds all possible states for each position and it makes explicit the independence assumption: each position only\n",
    "depends on the previous position. Here, each column represents a position in the sequence and each row represents a possible state. The following figure shows the trellis for $x = \\text{walk walk shop clean}$\n",
    "\n",
    "\n",
    "<img src=\"../images_for_notebooks/day_2/hmm_trellis.png\" style=\"max-width:100%; width: 60%\">\n",
    "\n",
    "\n",
    "\n",
    "Considering the trellis representation, note that we can include the following information:\n",
    "- an initial probability to the arrows that depart from the start symbol;\n",
    "- a final probability} to the arrows that reach the stop symbol\n",
    "- a transition probability to the remaining arrows\n",
    "-  an emission probability to each circle, which is the probability that the observed symbol is emitted by that particular state.\n",
    "\n",
    "\n",
    "###  Posterior decoding\n",
    "\n",
    "picking the highest state posterior for each position $i$ in the sequence:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i^* = \\arg \\max_{y_i \\in \\Lambda} P(Y_i=y_i | X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{equation}\n",
    " \n",
    "Note, however, that this approach does not guarantee that the sequence $y^*=y_1^* \\ldots y_N^*$ will be a\n",
    "valid sequence of the model. For instance, there might be a transition\n",
    "between two of the best state posteriors with probability zero. \n",
    "\n",
    "### Viterbi decoding\n",
    "\n",
    "consists in\n",
    "picking the best global hidden state sequence: \n",
    "\n",
    "\\begin{eqnarray}\n",
    "y^* &=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N | X_1=x_1,\\ldots,X_N =x_N)\\nonumber\\\\\n",
    "&=& \\text{argmax}_{y = y_1\\ldots y_N} P(Y_1=y_1,\\ldots, Y_N=y_N, X_1=x_1,\\ldots,X_N =x_N).\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viterbi decoding\n",
    "\n",
    "### Working with scores not probabilities\n",
    "\n",
    "For convenience, we will be working with \n",
    "log-probabilities, rather than probabilities. Therefore, if we associate to each circle and arrow in the trellis a score that corresponds\n",
    "to the log-probabilities above, and if we define the score of a path\n",
    "connecting the ${\\tt start}$ and  ${\\tt stop}$ symbols as\n",
    "the sum of the scores of the circles and arrows it traverses, \n",
    "then the goal of **finding the most likely sequence of states (Viterbi decoding) corresponds to finding the path with the highest score**.\n",
    "\n",
    "\n",
    "\n",
    "The trellis scores are given by the following expressions:\n",
    "\n",
    "- For each state $c_k$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{init}}(c_k) &=&\n",
    "\\log P_{\\mathrm{init}}(Y_{1} = c_k | \\text{start}).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each position $i \\in {1,\\ldots,N-1}$ and each pair of states $c_k$ and $c_l$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{trans}}(i, c_k, c_l) &=&\n",
    "\\log P_{\\mathrm{trans}}(Y_{i+1} = c_k | Y_i = c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each state $c_l$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{final}}(c_l) &=&\n",
    "\\log P_{\\mathrm{final}}(\\text{stop} | Y_N = c_l).\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "- For each position $i \\in {1,\\ldots,N}$ and state $c_k$:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\mathrm{score}_{\\mathrm{emiss}}(i, c_k) &=&\n",
    "\\log P_{\\mathrm{emiss}}(X_i = x_i | Y_i = c_k).\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The score of a path in the trellis is equivalent to the log-probability log P(x, y)\n",
    "\n",
    "Since the joint distribution $P_{\\theta} (X=x^m,Y=y^m)$ is given by the formula \n",
    "\n",
    "$$\n",
    "P(x,y)= \n",
    "P_{\\mathrm{init}}(y_1|\\text{start}) \n",
    "\\left(\n",
    "\\prod_{i=1}^{N-1} P_{\\mathrm{trans}}(y_{i+1}|y_i)\n",
    "\\right)\n",
    "P_{\\mathrm{final}}(\\text{stop}|y_N)\n",
    "\\prod_{i=1}^{N} P_{\\mathrm{emiss}}(x_i|y_i)\n",
    "$$\n",
    "\n",
    "when we apply the logarithm we get a sum of logarithms of 4 terms. Using the score notation defined above we get\n",
    "\n",
    "\n",
    "$$\n",
    "\\log P(x,y)= \\mathrm{score}_{\\mathrm{init}}(y_1) + \\sum_{i=1}^{N-1}\\mathrm{score}_{\\mathrm{trans}}(i, y_i, y_{i-1}) +\n",
    "\\mathrm{score}_{\\mathrm{final}}(c_l) +\n",
    " \\sum_{i=1}^{N} \\mathrm{score}_{\\mathrm{emiss}}(i, y_k) \n",
    "$$\n",
    "\n",
    "Since a path in the trellis is just an assignment of states $y=y_1,\\dots,y_N$ given words $x=x_1,\\dots,x_N$, computing the score of a path is just the sum of scores above. Moreover we have seen this is equivalent to computing the log probability of $(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Exercise 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convince yourself that the score of a path in the trellis (summing over the scores above) is equivalent to the log-probability \n",
    "log P(X = x, Y = y), as defined in Eq. 2.2. Use the given function compute scores on the first training sequence and confirm\n",
    "that the values are correct.\n",
    "You should get the same values as presented below.\n",
    "\n",
    "** Suggestion: use an example of length 5 instead of 4, emission_scores is a matrix of n_rows=len(sequence)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "# We will this append to ensure we can import lxmls toolking\n",
    "sys.path.append('../../lxmls-toolkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxmls.sequences.hmm as hmmc\n",
    "import lxmls.readers.simple_sequence as ssr\n",
    "simple = ssr.SimpleSequence()\n",
    "\n",
    "hmm = hmmc.HMM(simple.x_dict, simple.y_dict)\n",
    "hmm.train_supervised(simple.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_scores;\n",
      "[-0.40546511 -1.09861229] \n",
      "\n",
      "transition_scores: \n",
      "[[[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]\n",
      "\n",
      " [[-0.69314718        -inf]\n",
      "  [-0.69314718 -0.47000363]]] \n",
      "\n",
      "final_scores:\n",
      "[       -inf -0.98082925] \n",
      "\n",
      "emission_scores:\n",
      "[[-0.28768207 -1.38629436]\n",
      " [-0.28768207 -1.38629436]\n",
      " [-1.38629436 -0.98082925]\n",
      " [       -inf -0.98082925]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:172: RuntimeWarning: divide by zero encountered in log\n",
      "  transition_scores[pos-1, :, :] = np.log(self.transition_probs)\n",
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:170: RuntimeWarning: divide by zero encountered in log\n",
      "  emission_scores[pos, :] = np.log(self.emission_probs[sequence.x[pos], :])\n",
      "../../lxmls-toolkit/lxmls/sequences/hmm.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  final_scores = np.log(self.final_probs)\n"
     ]
    }
   ],
   "source": [
    "initial_scores, transition_scores, final_scores, emission_scores = hmm.compute_scores(simple.train.seq_list[1])\n",
    "\n",
    "print \"initial_scores;\\n\", initial_scores, \"\\n\"\n",
    "print \"transition_scores: \\n\",transition_scores, \"\\n\"\n",
    "print \"final_scores:\\n\", final_scores, \"\\n\"\n",
    "print \"emission_scores:\\n\", emission_scores, \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice a couple of things:\n",
    "\n",
    "- transition_scores is a matrix of shape (3,2,2), the first dimension corresponds to the len(x)-1\n",
    "    - The same matrix at each position is copied since the HMM is homogeneous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "walk/rainy walk/rainy shop/rainy clean/sunny "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple.train.seq_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emission Probabilities\n",
      "[[ 0.75   0.25 ]\n",
      " [ 0.25   0.375]\n",
      " [ 0.     0.375]\n",
      " [ 0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print \"Emission Probabilities\\n\", hmm.emission_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition Probabilities\n",
      "[[ 0.5    0.   ]\n",
      " [ 0.5    0.625]]\n"
     ]
    }
   ],
   "source": [
    "print \"transition Probabilities\\n\", hmm.transition_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **if emission_scores = log (emission_probabilities) why are there not 3 -inf????**\n",
    "- **Why we save length(x)-1 times the transition_scores??**\n",
    "\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        length = len(sequence.x) # Length of the sequence.\n",
    "        num_states = self.get_num_states() # Number of states of the HMM.\n",
    "\n",
    "        # Initial position.\n",
    "        initial_scores = np.log(self.initial_probs)\n",
    "\n",
    "        # Intermediate position.\n",
    "        # logzero is just -np.inf\n",
    "        emission_scores = np.zeros([length, num_states]) + logzero()\n",
    "        transition_scores = np.zeros([length-1, num_states, num_states]) + logzero()\n",
    "        for pos in xrange(length):\n",
    "            import pdb;pdb.set_trace()\n",
    "            emission_scores[pos,:] = np.log(self.emission_probs[sequence.x[pos], :])\n",
    "            if pos > 0:\n",
    "                transition_scores[pos-1,:,:] = np.log(self.transition_probs)\n",
    "\n",
    "        # Final position.\n",
    "        final_scores = np.log(self.final_probs)\n",
    "\n",
    "        return initial_scores, transition_scores, final_scores, emission_scores\n",
    "        \n",
    "        \n",
    "Could be changed to\n",
    "\n",
    "\n",
    "    def compute_scores(self, sequence):\n",
    "        length = len(sequence.x) # Length of the sequence.\n",
    "        num_states = self.get_num_states() # Number of states of the HMM.\n",
    "\n",
    "        # Initial position.\n",
    "        initial_scores = np.log(self.initial_probs)\n",
    "\n",
    "        # Intermediate positions\n",
    "        transition_scores = np.log(self.transition_probs) ## now we don't copy the matrix per position\n",
    "        emission_scores = np.log(self.emission_probs[sequence.x,:])\n",
    "        \n",
    "        # Final position.\n",
    "        final_scores = np.log(self.final_probs)\n",
    "\n",
    "        return initial_scores, transition_scores, final_scores, emission_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computations in log-domain\n",
    "\n",
    "We will see that the decoding algorithms \n",
    "will need to multiply twice as many probability terms as \n",
    "the length $N$ of the sequence. \n",
    "This may cause underflowing problems \n",
    "when $N$ is large, since the nested multiplication of numbers smaller than 1 may easily become smaller than the machine precision. To avoid that\n",
    "problem,  presents a scaled version of the decoding algorithms that avoids this problem. An alternative, which is widely used, is computing\n",
    "in the log-domain. That is, instead of \n",
    "manipulating probabilities, manipulate log-probabilities (the scores presented above). \n",
    "\n",
    "Every time we need to multiply probabilities, \n",
    "we can sum their log-representations, since:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log(\\exp(a) \\times \\exp(b)) = a+b.\n",
    "\\end{equation}\n",
    "\n",
    "Sometimes, we need to add probabilities. \n",
    "In the log domain, this requires us to compute \n",
    "\n",
    "\\begin{equation}\n",
    "\\log(\\exp(a) + \\exp(b)) = a + \\log(1 + \\exp(b-a)),\n",
    "\\end{equation}\n",
    "\n",
    "where we assume that $a$ is smaller than $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the module ``sequences/log_domain.py.`` This module implements a function ```logsum_pair(logx, logy)``` to add two numbers\n",
    "represented in the log-domain; it returns their sum also represented in the log-domain.\n",
    "\n",
    "The function ```logsum(logv)``` sums all components of an array represented in the log-domain.\n",
    "This will be used later in our decoding algorithms. To observe why this is important, type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.81314156932\n",
      "9.44818137728\n",
      "88.0957187891\n",
      "inf\n",
      "\n",
      "\n",
      "2.81314156932\n",
      "9.44818137728\n",
      "88.0957187891\n",
      "880.931234995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dferreira/.pyenv/versions/2.7.12/lib/python2.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.rand(10)\n",
    "print np.log(sum(np.exp(a)))\n",
    "print np.log(sum(np.exp(10*a)))\n",
    "print np.log(sum(np.exp(100*a)))\n",
    "print np.log(sum(np.exp(1000*a)))\n",
    "\n",
    "print \"\\n\"\n",
    "from lxmls.sequences.log_domain import logsum\n",
    "print logsum(a)\n",
    "print logsum(10*a)\n",
    "print logsum(100*a)\n",
    "print logsum(1000*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
